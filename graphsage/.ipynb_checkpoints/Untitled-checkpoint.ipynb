{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preload import load_data\n",
    "from model import *\n",
    "# from edge import *\n",
    "from utils import *\n",
    "from sampler import *\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "root_path = os.getcwd() + '/'\n",
    "\n",
    "dataset = 'reddit'\n",
    "batch_size = 256\n",
    "node_iters = 20\n",
    "emb_size = 128\n",
    "node_early_stop = 20\n",
    "node_least_iter = 100\n",
    "num_neigh0=5\n",
    "num_neigh1=5\n",
    "is_cuda = False\n",
    "if dataset=='pubmed' or dataset=='reddit':\n",
    "    batch_size=128\n",
    "    num_neigh0 = 25\n",
    "    num_neigh1 = 10\n",
    "    node_least_iter = 80\n",
    "batch_size = 4\n",
    "\n",
    "eva_size = 16\n",
    "\n",
    "model_name = 'model_save/' + dataset + '.pkl'\n",
    "\n",
    "eva_iter = 4\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preload import load_data, load_reddit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj, features, labels, train_index, val_index, test_index = load_reddit_data(cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 nodes completed.\n",
      "10000 nodes completed.\n",
      "20000 nodes completed.\n",
      "30000 nodes completed.\n",
      "40000 nodes completed.\n",
      "50000 nodes completed.\n",
      "60000 nodes completed.\n",
      "70000 nodes completed.\n",
      "80000 nodes completed.\n",
      "90000 nodes completed.\n",
      "100000 nodes completed.\n",
      "110000 nodes completed.\n",
      "120000 nodes completed.\n",
      "130000 nodes completed.\n",
      "140000 nodes completed.\n",
      "150000 nodes completed.\n",
      "160000 nodes completed.\n",
      "170000 nodes completed.\n",
      "180000 nodes completed.\n",
      "190000 nodes completed.\n",
      "200000 nodes completed.\n",
      "210000 nodes completed.\n",
      "220000 nodes completed.\n",
      "230000 nodes completed.\n",
      "Compute sampler in 0.64 min\n"
     ]
    }
   ],
   "source": [
    "s = SamplerReddit(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chenhao/PyProjects/LAGCN_whole/graphsage/model.py:235: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  init.xavier_uniform(self.weight)\n"
     ]
    }
   ],
   "source": [
    "sampler = s.normal_sample\n",
    "num_nodes = features.shape[0]\n",
    "num_feature = features.shape[1]\n",
    "num_classes = labels.shape[1]\n",
    "feat = nn.Embedding(num_nodes, num_feature)\n",
    "feat.weight = nn.Parameter(torch.FloatTensor(features), requires_grad=False)\n",
    "\n",
    "agg1 = MeanAggregator(feat, sampler, is_softgate=True, cuda=is_cuda)\n",
    "enc1 = Encoder(feat, num_feature, emb_size, agg1, gcn=False, cuda=is_cuda)\n",
    "agg2 = MeanAggregator(\n",
    "    lambda nodes: enc1(nodes).t(), sampler, is_softgate=True, cuda=is_cuda)\n",
    "enc2 = Encoder(lambda nodes: enc1(nodes).t(), enc1.embed_dim, emb_size, agg2,\n",
    "    base_model=enc1, gcn=False, cuda=is_cuda)\n",
    "enc1.num_sample = num_neigh1\n",
    "enc2.num_sample = num_neigh0\n",
    "graphsage = SupervisedGraphSage(num_classes, enc2, drop_out=0.5, cuda=is_cuda)\n",
    "optimizer = torch.optim.SGD(filter(lambda p: p.requires_grad, graphsage.parameters()), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_eva(net, x, y, batch_size):\n",
    "    out_list = []\n",
    "    for idx in range(0, len(x),batch_size):\n",
    "        end_idx = min(idx+batch_size,len(x))\n",
    "        batch_nodes = x[idx:end_idx]\n",
    "        output = net.forward(batch_nodes).data.cpu().numpy().argmax(axis=1) \n",
    "        out_list.append(output)\n",
    "    pred = np.concatenate(out_list,axis=0)\n",
    "    acc = accuracy_score(y,pred)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, |Pat: 0/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0625\n",
      "Epoch: 4, |Pat: 1/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Epoch: 8, |Pat: 2/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Epoch: 12, |Pat: 3/5 |loss: 3.7137 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Epoch: 16, |Pat: 4/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0625\n",
      "Epoch: 20, |Pat: 0/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.1250\n",
      "Epoch: 24, |Pat: 1/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Epoch: 28, |Pat: 0/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.1875\n",
      "Epoch: 32, |Pat: 1/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0625\n",
      "Epoch: 36, |Pat: 2/5 |loss: 75.8382 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Epoch: 40, |Pat: 3/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0625\n",
      "Epoch: 44, |Pat: 4/5 |loss: 3.7136 |tr_acc: 0.0000 |va_acc: 0.0000\n",
      "Early Stop\n",
      "******************** Final Test ********************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "batch_eva() missing 4 required positional arguments: 'net', 'x', 'y', and 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3aab4ab52e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Final Test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'*'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mgraphsage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mts_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_eva\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test Acc:%.4f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mts_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: batch_eva() missing 4 required positional arguments: 'net', 'x', 'y', and 'batch_size'"
     ]
    }
   ],
   "source": [
    "score_list = []\n",
    "times = []\n",
    "patience_count=0\n",
    "max_score=0\n",
    "for batch in range(100000):\n",
    "    batch_nodes = random.sample(list(train_index), batch_size)\n",
    "    start_time = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    label = Variable(torch.LongTensor(labels[batch_nodes].argmax(axis=1)))\n",
    "    if is_cuda:\n",
    "        label = label.cuda()\n",
    "    tr_output, loss = graphsage.loss(\n",
    "        batch_nodes,\n",
    "        label,\n",
    "        is_train=True\n",
    "    )\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "    if batch%eva_iter ==0:\n",
    "        va_nodes = random.sample(list(val_index), eva_size)\n",
    "        tr_acc = batch_eva(graphsage,batch_nodes,labels[batch_nodes].argmax(axis=1),batch_size)\n",
    "        va_acc = batch_eva(graphsage,va_nodes,labels[va_nodes].argmax(axis=1),batch_size)\n",
    "        if va_acc > max_score:\n",
    "            max_score=va_acc\n",
    "            patience_count = 0\n",
    "            torch.save(graphsage.state_dict(), model_name)\n",
    "        else:\n",
    "            patience_count +=1\n",
    "            if patience_count==patience:\n",
    "                print('Early Stop')\n",
    "                break\n",
    "        print('Epoch: %d,' %(batch),\n",
    "          '|Pat: %d/%d' % (patience_count, patience),\n",
    "          '|loss: %.4f' % loss.data.numpy(),\n",
    "          '|tr_acc: %.4f' % tr_acc,\n",
    "          '|va_acc: %.4f' % va_acc)\n",
    "print('*'*20, 'Final Test', '*'*20)\n",
    "graphsage.load_state_dict(torch.load(model_name))  \n",
    "ts_acc = batch_eva(graphsage,test_index,labels[test_index].argmax(axis=1),batch_size)\n",
    "print('Test Acc:%.4f'%ts_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
